from pyspark.sql import SparkSession
import numpy as np
import pandas as pd

# Create a Spark session
spark = (
    SparkSession.builder
        .master("local")
        .appName("Employee Data")
        .getOrCreate()
)

np.random.seed(42)  # For reproducibility
data = {
    'EmployeeID': np.arange(1, 51),
    'Name': [f'Employee_{i}' for i in range(1, 51)],
    'Department': np.random.choice(['HR', 'Finance', 'Engineering', 'Marketing', 'Sales'], 50),
    'Age': np.random.randint(22, 60, size=50),
    'Gender': np.random.choice(['M', 'F'], 50),
    'Salary': np.random.randint(40000, 120000, size=50),
    'YearsAtCompany': np.random.randint(1, 20, size=50)
}

# Create a Pandas DataFrame
employee_df = pd.DataFrame(data)

# Display the first few rows of the Pandas DataFrame
print(employee_df.head())

# Convert the Pandas DataFrame to a PySpark DataFrame
spark_df = spark.createDataFrame(employee_df)

# Show the PySpark DataFrame
spark_df.show()
